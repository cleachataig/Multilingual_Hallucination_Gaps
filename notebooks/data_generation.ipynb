{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to analyze our generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import json\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import unquote\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import py3langid as langid\n",
    "import config\n",
    "import tikzplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='../data/'\n",
    "result_dir='../result/generation/'\n",
    "fig_dir='../report/Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages=config.languages\n",
    "models=list(config.checkpoints.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA IMPORTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=['en', 'lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations={}\n",
    "for m in models:\n",
    "    generations[m]={}\n",
    "    for p in prompt:\n",
    "        with open(result_dir+f'biographies_{m}_{p}.json', 'r') as f:\n",
    "            generations[m][p]=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove model-specific tokens from the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tokens = {\n",
    "        'aya': '<|CHATBOT_TOKEN|>',\n",
    "        'llama': 'assistant\\n\\n',\n",
    "        'qwen': '\\nassistant\\n'\n",
    "    }\n",
    "\n",
    "def clean_generation(raw_text, model):\n",
    "    answer=raw_text.split(split_tokens[model])[1]\n",
    "    answer=answer.replace('\\n', '')\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    model_type=m.split('_')[0]\n",
    "    for p in prompt:\n",
    "        new_gen_dict={}\n",
    "        for link, lang_dict in generations[m][p].items():\n",
    "            new_gen_dict[link]={}\n",
    "            for lang_code, gen_list in lang_dict.items():\n",
    "                new_gen_dict[link][lang_code]=[clean_generation(gen, model_type) for gen in gen_list]\n",
    "        generations[m][p]=new_gen_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANITY CHECK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the sanity checks (correct language and enough unique words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py3lang_detect(text):\n",
    "    '''Detect the language of the provided text, as well as other probable languages'''\n",
    "    lang, prob = langid.classify(text)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(sentence):\n",
    "    sentence = ''.join(char.lower() if char.isalnum() or char.isspace() else ' ' for char in sentence)\n",
    "    words = sentence.split()\n",
    "    unique_words = set(words)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 1/12 [00:21<03:51, 21.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For llama_8 en, 23181 generations left (83.67987870911847%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 2/12 [00:46<03:56, 23.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For llama_8 lang, 24516 generations left (88.4990253411306%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 3/12 [01:08<03:24, 22.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For llama_70 en, 22590 generations left (81.54645873944119%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 4/12 [01:34<03:13, 24.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For llama_70 lang, 26306 generations left (94.96065266045774%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 5/12 [01:57<02:45, 23.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For qwen_7 en, 24866 generations left (89.7624720236806%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 6/12 [02:19<02:19, 23.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For qwen_7 lang, 24678 generations left (89.08382066276803%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 7/12 [02:42<01:55, 23.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For qwen_72 en, 20632 generations left (74.47837701249007%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 8/12 [03:06<01:33, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For qwen_72 lang, 24778 generations left (89.44480542921089%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 9/12 [03:29<01:09, 23.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For aya_8 en, 19610 generations left (70.78911269944408%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 10/12 [03:55<00:48, 24.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For aya_8 lang, 22278 generations left (80.42018626813949%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 11/12 [04:22<00:24, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For aya_35 en, 23414 generations left (84.52097321493032%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 12/12 [04:48<00:00, 24.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For aya_35 lang, 23229 generations left (83.85315139701105%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_generations={}\n",
    "rows=[]\n",
    "with tqdm(total=len(models)*len(prompt), desc=\"Processing\") as pbar:\n",
    "    for m in models:\n",
    "        clean_generations[m]={}\n",
    "        for p in prompt:\n",
    "            count=0\n",
    "            clean_generations[m][p]={}\n",
    "            for link, lang_dict in generations[m][p].items():\n",
    "                new_lang_dict=lang_dict.copy()\n",
    "                for lang_code, gen_list in lang_dict.items():\n",
    "                    new_gen_list=gen_list.copy()\n",
    "                    for gen in gen_list:\n",
    "                        pred_lang = py3lang_detect(gen)\n",
    "                        nb_words=len(unique_words(gen))\n",
    "                        new_row = {'model':m, 'prompt':p, 'link': link, 'gold_lang': lang_code, 'pred_lang':pred_lang, 'tokens': len(gen), 'unique_tokens': nb_words}\n",
    "                        rows.append(new_row)\n",
    "                        if pred_lang != lang_code or nb_words<20:\n",
    "                            new_gen_list.remove(gen)\n",
    "                    if len(new_gen_list)==0:\n",
    "                        new_lang_dict.pop(lang_code)\n",
    "                    else:\n",
    "                        new_lang_dict[lang_code]=new_gen_list\n",
    "                        count+=len(new_gen_list)\n",
    "                clean_generations[m][p][link]=new_lang_dict\n",
    "            print(f\"For {m} {p}, {count} generations left ({(count/(19*486*3))*100}%)\")\n",
    "            pbar.update(1)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    for p in prompt:\n",
    "        with open(result_dir+f'/clean_biographies_{m}_{p}.json', 'w') as f:\n",
    "            json.dump(clean_generations[m][p], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df=pd.DataFrame(rows, columns=['model', 'prompt', 'link', 'gold_lang', 'pred_lang', 'tokens', 'unique_tokens'])\n",
    "lang_df.to_csv(result_dir+f'lang_dectect.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze the results of the sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df=pd.read_csv(result_dir+'lang_dectect.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build new columns for lang matching and number of tokens\n",
    "lang_df['match'] = (lang_df['gold_lang'] == lang_df['pred_lang']).astype(int)\n",
    "lang_df['tokens'] = lang_df['tokens'].astype(int)\n",
    "lang_df['unique_tokens'] = lang_df['unique_tokens'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8621910572040526"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Percentages of generations in correct language:', sum(lang_df.match)/len(lang_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot percentage of generations in correct language per subject LLM\n",
    "\n",
    "aggregated_df=lang_df.groupby(['model', 'prompt', 'gold_lang']).agg(match_norm=('match', lambda x: x.sum() / x.count()))\n",
    "aggregated_df = aggregated_df.reset_index()\n",
    "pivoted_df = aggregated_df.pivot_table(\n",
    "    index='gold_lang',\n",
    "    columns=['model', 'prompt'],\n",
    "    values='match_norm'\n",
    ")\n",
    "pivoted_df.columns = ['_'.join(col).strip() for col in pivoted_df.columns.values]\n",
    "pivoted_df=pivoted_df.reindex(languages)\n",
    "pivoted_df=pivoted_df[['aya_8_lang', 'aya_8_en', 'aya_35_lang', 'aya_35_en', \n",
    "                       'llama_8_lang', 'llama_8_en', 'llama_70_lang','llama_70_en',\n",
    "                       'qwen_7_lang', 'qwen_7_en', 'qwen_72_lang' , 'qwen_72_en']]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivoted_df, cmap=\"coolwarm_r\", linewidths=.5)\n",
    "plt.title('Heatmap of Normalized Match Scores Across Models and Prompts')\n",
    "plt.xlabel('Model_Prompt')\n",
    "plt.ylabel('Gold Language')\n",
    "tikzplotlib.save(fig_dir+\"langdetect.tex\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "respai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
